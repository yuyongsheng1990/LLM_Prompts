{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b8f32d3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T01:46:54.076030Z",
     "start_time": "2024-07-02T01:46:54.071853Z"
    }
   },
   "outputs": [],
   "source": [
    "# https://github.com/cccntu/minLoRA/blob/main/demo.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d76a3c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-01T08:56:51.441984Z",
     "start_time": "2024-07-01T08:56:51.438291Z"
    }
   },
   "source": [
    "## Minilora Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3878563",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T01:46:58.383508Z",
     "start_time": "2024-07-02T01:46:55.001528Z"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from functools import partial  # 用于固定某些函数的参数，从而创建一个新的函数。这个新函数会记住被固定的参数，并在调用时使用这些固定参数。\n",
    "\n",
    "import torch\n",
    "import torch.nn.utils.parametrize as parametrize\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af0c279",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-01T09:14:23.804481Z",
     "start_time": "2024-07-01T09:14:23.801155Z"
    }
   },
   "source": [
    "### lora class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d76602e9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T01:46:59.803802Z",
     "start_time": "2024-07-02T01:46:59.792916Z"
    }
   },
   "outputs": [],
   "source": [
    "class LoRAParametrization(nn.Module):\n",
    "    def __init__(self, fan_in, fan_out, fan_in_fan_out=False, rank=4, lora_dropout_p=0.0, lora_alpha=1):\n",
    "        super().__init__()\n",
    "        # if weight is stored as (fan_out, fan_in), the memory layout of A & B follows (W + BA)x\n",
    "        # otherwise, it's x(W + AB). This allows us to tie the weights between linear layers and embeddings\n",
    "        self.swap = (lambda x: (x[1], x[0])) if fan_in_fan_out else (lambda x: x)\n",
    "        self.lora_A = nn.Parameter(torch.zeros(self.swap((rank, fan_in))))\n",
    "        self.lora_B = nn.Parameter(torch.zeros(self.swap((fan_out, rank))))\n",
    "        nn.init.kaiming_uniform_(self.lora_A, a=math.sqrt(5))\n",
    "        self.lora_alpha, self.rank = lora_alpha, rank\n",
    "        self.scaling = lora_alpha / rank\n",
    "        self.lora_dropout = nn.Dropout(p=lora_dropout_p) if lora_dropout_p > 0 else lambda x: x\n",
    "        self.dropout_fn = self._dropout if lora_dropout_p > 0 else lambda x: x\n",
    "        self.register_buffer(\"lora_dropout_mask\", torch.ones(self.swap((1, fan_in)), dtype=self.lora_A.dtype))\n",
    "        self.forward_fn = self.lora_forward\n",
    "\n",
    "    def _dropout(self, A):\n",
    "        # to mimic the original implementation: A @ dropout(x), we do (A * dropout(ones)) @ x\n",
    "        return A * self.lora_dropout(self.lora_dropout_mask)\n",
    "\n",
    "    def lora_forward(self, X):\n",
    "        return X + torch.matmul(*self.swap((self.lora_B, self.dropout_fn(self.lora_A)))).view(X.shape) * self.scaling\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.forward_fn(X)\n",
    "\n",
    "    def disable_lora(self):\n",
    "        self.forward_fn = lambda x: x  # 输出等于输入x，这种设置通常用于临时禁用lora，表示不会在调用lora_forward,model(x)会使用原线性层计算。\n",
    "\n",
    "    def enable_lora(self):\n",
    "        self.forward_fn = self.lora_forward\n",
    "\n",
    "    @classmethod  # 类方法装饰器，它们不绑定到实例上，而是绑定到类本身。\n",
    "    def from_linear(cls, layer, rank=4, lora_dropout_p=0.0, lora_alpha=1):\n",
    "        fan_out, fan_in = layer.weight.shape  # (5，7),从layer.weight属性中提出fan_in和fan_out。\n",
    "        return cls(  # cls是class缩写，表示当前类本身。这里cls被用来调用构造函数__init__，返回一个新的MyLoRA实例\n",
    "            fan_in, fan_out, fan_in_fan_out=False, rank=rank, lora_dropout_p=lora_dropout_p, lora_alpha=lora_alpha\n",
    "        )\n",
    "\n",
    "    @classmethod\n",
    "    def from_conv2d(cls, layer, rank=4, lora_dropout_p=0.0, lora_alpha=1):\n",
    "        fan_out, fan_in = layer.weight.view(layer.weight.shape[0], -1).shape\n",
    "        return cls(\n",
    "            fan_in, fan_out, fan_in_fan_out=False, rank=rank, lora_dropout_p=lora_dropout_p, lora_alpha=lora_alpha\n",
    "        )\n",
    "\n",
    "    @classmethod\n",
    "    def from_embedding(cls, layer, rank=4, lora_dropout_p=0.0, lora_alpha=1):\n",
    "        fan_in, fan_out = layer.weight.shape\n",
    "        return cls(\n",
    "            fan_in, fan_out, fan_in_fan_out=True, rank=rank, lora_dropout_p=lora_dropout_p, lora_alpha=lora_alpha\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b00781",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-01T09:13:27.671442Z",
     "start_time": "2024-07-01T09:13:27.668198Z"
    }
   },
   "source": [
    "### lora config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bbe861f7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T06:20:16.261497Z",
     "start_time": "2024-07-02T06:20:16.257336Z"
    }
   },
   "outputs": [],
   "source": [
    "default_lora_config = {  # specify which layers to add lora to, by default only add to linear layers\n",
    "    nn.Linear: {  # lora_config的key = '<class 'torch.nn.modules.linear.Linear'>'\n",
    "        # 表示单个参数参数化方法\n",
    "        \"weight\": partial(LoRAParametrization.from_linear, rank=4),  # key='weight', value=partial固定部分参数的单层LoRA layer\n",
    "        # 可以顺序应用多个参数化方法，继续对应DoRA\n",
    "        # \"weight\": partial(MultiplyByTwoParametrization.from_linear, rank=3),\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3881a0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-01T09:13:40.879222Z",
     "start_time": "2024-07-01T09:13:40.874952Z"
    }
   },
   "source": [
    "### lora functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2a9873f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T01:47:20.719878Z",
     "start_time": "2024-07-02T01:47:20.711872Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n    lora_config:  {<class 'torch.nn.modules.linear.Linear'>: {'weight': functools.partial(<bound method \\n    LoRAParametrization.from_linear of <class '__main__.LoRAParametrization'>>, rank=4)}}\\n    type(layer):  <class 'torch.nn.modules.linear.Linear'>\\n    ----------\\n    contained\\n    type(layer):  <class 'torch.nn.modules.linear.Linear'>\\n    ----------\\n    contained\\n    type(layer):  <class 'torch.nn.modules.container.Sequential'>\\n    ----------\\n\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "    lora_config:  {<class 'torch.nn.modules.linear.Linear'>: {'weight': functools.partial(<bound method \n",
    "    LoRAParametrization.from_linear of <class '__main__.LoRAParametrization'>>, rank=4)}}\n",
    "    type(layer):  <class 'torch.nn.modules.linear.Linear'>\n",
    "    ----------\n",
    "    contained\n",
    "    type(layer):  <class 'torch.nn.modules.linear.Linear'>\n",
    "    ----------\n",
    "    contained\n",
    "    type(layer):  <class 'torch.nn.modules.container.Sequential'>\n",
    "    ----------\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d022d650",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T06:23:47.331620Z",
     "start_time": "2024-07-02T06:23:47.319160Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=5, out_features=5, bias=True)\n",
      "ParametrizedLinear(\n",
      "  in_features=5, out_features=5, bias=True\n",
      "  (parametrizations): ModuleDict(\n",
      "    (weight): ParametrizationList(\n",
      "      (0): LowRankParametrization()\n",
      "      (1): MultiplyByTwoParametrization()\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\noutput:\\n    Linear(in_features=5, out_features=5, bias=True)  # 原始linear层\\n    -------------------------------------------------\\n    ParametrizedLinear(                          # 替换后的参数化线性层para linear\\n      in_features=5, out_features=5, bias=True   # 这表示layer原始参数original weight\\n      (parametrizations): ModuleDict(            # parametrizations表示应用参数化方法，新模型参数会存储在ModuleDict中，ModuleDict是一个module容器，它像一个dict一样工作。\\n        (weight): ParametrizationList(           # 这表示weight原始参数现在被替换/应用了ParametrizationList中一个或多个参数化方法.\\n          (0): LowRankParametrization()          # (0)表示ParametrizationList的第一个参数化方法。\\n        )                                        # 顺序应用：当ParametrizationList存储多个参数化方法时，所有方法会按顺序应用到weight参数上。\\n      )\\n    )\\n'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "simple example: torch.nn.utils.parametrize.register_parametrization\n",
    "    output: 原始参数(weight或bias)会被替换为一个通过指定参数模块生成的参数。\n",
    "    Linear(\n",
    "      (weight): ParametrizationList(\n",
    "        (0): MyParametrization()\n",
    "      )\n",
    "      (bias): Parameter containing: [torch.FloatTensor of size 5]\n",
    "    )\n",
    "'''\n",
    "linear = nn.Linear(5, 5)\n",
    "print(linear)\n",
    "class LowRankParametrization(nn.Module):\n",
    "    def __init__(self, original_weight, rank=4):\n",
    "        super().__init__()\n",
    "        self.rank = rank\n",
    "        self.U = nn.Parameter(torch.randn(original_weight.size(0), rank))\n",
    "        self.V = nn.Parameter(torch.randn(rank, original_weight.size(1)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.U @ self.V\n",
    "\n",
    "# 注册低秩参数化\n",
    "'''\n",
    "    torch.nn.utils.parametrize.register_parametrization函数用于在模型的参数上注册新的参数化方法。\n",
    "    这个功能允许你在现有参数layer.weight上应用一些变换LoRAParametrization，特别适用于LoRA\n",
    "'''\n",
    "parametrize.register_parametrization(linear, 'weight', LowRankParametrization(linear.weight))\n",
    "# 可以顺序应用多个参数化方法，继续加就行 <--对应DoRA\n",
    "# 定义第二个参数化方法\n",
    "class MultiplyByTwoParametrization(nn.Module):\n",
    "    def __init__(self, original_weight, rank=4):\n",
    "        super().__init__()\n",
    "        self.rank = rank\n",
    "        self.U = nn.Parameter(torch.randn(original_weight.size(0), rank))\n",
    "        self.V = nn.Parameter(torch.randn(rank, original_weight.size(1)))\n",
    "    def forward(self, x):\n",
    "        return self.U @ self.V\n",
    "parametrize.register_parametrization(linear, 'weight', MultiplyByTwoParametrization(linear.weight, rank=3))\n",
    "    \n",
    "# 打印线性层，查看参数化后的结果\n",
    "print(linear)\n",
    "'''\n",
    "output:\n",
    "    Linear(in_features=5, out_features=5, bias=True)  # 原始linear层\n",
    "    -------------------------------------------------\n",
    "    ParametrizedLinear(                          # 替换后的参数化线性层para linear\n",
    "      in_features=5, out_features=5, bias=True   # 这表示layer原始参数original weight\n",
    "      (parametrizations): ModuleDict(            # parametrizations表示应用参数化方法，新模型参数会存储在ModuleDict中，ModuleDict是一个module容器，它像一个dict一样工作。\n",
    "        (weight): ParametrizationList(           # 这表示weight原始参数现在被替换/应用了ParametrizationList中一个或多个参数化方法.\n",
    "          (0): LowRankParametrization()          # (0)表示ParametrizationList的第一个参数化方法。\n",
    "        # (1): MultiplyByTwoParametrization()    # 顺序应用：当ParametrizationList存储多个参数化方法时，所有方法会按顺序应用到weight参数上。\n",
    "        )                                        \n",
    "      )\n",
    "    )\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "47fe6245",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T07:49:49.020155Z",
     "start_time": "2024-07-02T07:49:49.012178Z"
    }
   },
   "outputs": [],
   "source": [
    "def apply_lora(layer, register=True, merge=False, lora_config=default_lora_config): # layer=simple model\n",
    "    \"\"\"add lora parametrization to a layer, designed to be used with model.apply\"\"\"\n",
    "    print('Original layer: ', type(layer))\n",
    "    if register:\n",
    "        if type(layer) in lora_config:\n",
    "            for attr_name, parametrization in lora_config[type(layer)].items():  # items函数以list形式返回(key,value)元组列表。\n",
    "                # torch.nn.utils.parametrize.register_parametrization函数用于在模型的参数上注册新的参数化方法。\n",
    "                # 这个功能允许你在现有参数layer.weight上应用一些变换LoRAParametrization，特别适用于LoRA\n",
    "                parametrize.register_parametrization(layer, attr_name, parametrization(layer))  # LoRAParametrization\n",
    "                print('LoRA Layer: ', type(layer))\n",
    "                print('-'*20)\n",
    "    else:  # this will remove all parametrizations, use with caution\n",
    "        if hasattr(layer, \"parametrizations\"):\n",
    "            for attr_name in layer.parametrizations.keys():\n",
    "                parametrize.remove_parametrizations(layer, attr_name, leave_parametrized=merge)\n",
    "\n",
    "# simple model将lora.linear的参数传递给apply_lora函数。\n",
    "def add_lora(model, lora_config=default_lora_config):  # lora_config是一个dict，key=nn.Linear; value={'weight':lora layer}\n",
    "    \"\"\"add lora parametrization to all layers in a model. Calling it twice will add lora twice\"\"\"\n",
    "    print('lora_config: ', lora_config)\n",
    "    model.apply(partial(apply_lora, lora_config=lora_config))  # model每一层应用--固定参数的apply_lora函数\n",
    "\n",
    "\n",
    "def add_lora_by_name(model, target_module_names, lora_config=default_lora_config):\n",
    "    \"\"\"Add LoRA parameterization to specific layers in a model by names\"\"\"\n",
    "    for name, layer in model.named_modules():\n",
    "        if any([m in name for m in target_module_names]):\n",
    "            add_lora(layer, lora_config=lora_config)\n",
    "\n",
    "\n",
    "def merge_lora(model):\n",
    "    \"\"\"merge lora parametrization to all layers in a model. This will remove all parametrization\"\"\"\n",
    "    model.apply(partial(apply_lora, register=False, merge=True))\n",
    "\n",
    "\n",
    "def remove_lora(model):\n",
    "    \"\"\"remove lora parametrization to all layers in a model. This will remove all parametrization\"\"\"\n",
    "    model.apply(partial(apply_lora, register=False, merge=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2706616e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T06:47:32.604493Z",
     "start_time": "2024-07-02T06:47:32.600991Z"
    }
   },
   "source": [
    "### utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "34fd9557",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T06:47:49.684728Z",
     "start_time": "2024-07-02T06:47:49.680432Z"
    }
   },
   "outputs": [],
   "source": [
    "def apply_to_lora(fn):\n",
    "    \"\"\"apply a function to LoRAParametrization layers, designed to be used with model.apply\"\"\"\n",
    "\n",
    "    def apply_fn(layer):\n",
    "        if isinstance(layer, LoRAParametrization):\n",
    "            fn(layer)\n",
    "\n",
    "    return apply_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7a3a8bbc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T06:58:18.500335Z",
     "start_time": "2024-07-02T06:58:18.496527Z"
    }
   },
   "outputs": [],
   "source": [
    "enable_lora = lambda model: model.apply(apply_to_lora(lambda x: x.enable_lora()))\n",
    "disable_lora = lambda model: model.apply(apply_to_lora(lambda x: x.disable_lora()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "69b33cae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T09:36:06.934545Z",
     "start_time": "2024-07-02T09:36:06.922636Z"
    }
   },
   "outputs": [],
   "source": [
    "def name_is_lora(name):\n",
    "    return (\n",
    "        len(name.split(\".\")) >= 4\n",
    "        and (name.split(\".\")[-4]) == \"parametrizations\"\n",
    "        and name.split(\".\")[-1] in [\"lora_A\", \"lora_B\"]\n",
    "    )\n",
    "\n",
    "\n",
    "def name_is_bias(name):\n",
    "    return name.split(\".\")[-1] == \"bias\"\n",
    "\n",
    "\n",
    "def get_params_by_name(model, print_shapes=False, name_filter=None):\n",
    "    for n, p in model.named_parameters():\n",
    "        if name_filter is None or name_filter(n):\n",
    "            if print_shapes:\n",
    "                print(n, p.shape)\n",
    "            yield p\n",
    "\n",
    "\n",
    "def get_lora_params(model, print_shapes=False):\n",
    "    return get_params_by_name(model, print_shapes=print_shapes, name_filter=name_is_lora)\n",
    "\n",
    "\n",
    "def get_bias_params(model, print_shapes=False):\n",
    "    return get_params_by_name(model, print_shapes=print_shapes, name_filter=name_is_bias)\n",
    "\n",
    "\n",
    "def get_lora_state_dict(model):\n",
    "    return {k: v for k, v in model.state_dict().items() if name_is_lora(k)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "13cc0eb8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T09:00:06.565482Z",
     "start_time": "2024-07-02T09:00:06.560155Z"
    }
   },
   "outputs": [],
   "source": [
    "def _prepare_for_multiple_lora(lora_layer):\n",
    "    lora_layer.lora_As = []\n",
    "    lora_layer.lora_Bs = []\n",
    "\n",
    "\n",
    "def _append_lora(lora_layer):\n",
    "    lora_layer.lora_As.append(nn.Parameter(lora_layer.lora_A.clone()))\n",
    "    lora_layer.lora_Bs.append(nn.Parameter(lora_layer.lora_B.clone()))\n",
    "\n",
    "\n",
    "def load_multiple_lora(model, lora_state_dicts):\n",
    "    model.apply(apply_to_lora(_prepare_for_multiple_lora))\n",
    "    for state_dict in lora_state_dicts:\n",
    "        _ = model.load_state_dict(state_dict, strict=False)\n",
    "        model.apply(apply_to_lora(_append_lora))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "d0d088bb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T09:14:23.451140Z",
     "start_time": "2024-07-02T09:14:23.446441Z"
    }
   },
   "outputs": [],
   "source": [
    "def _select_lora(lora_layer, index):\n",
    "    lora_layer.lora_A = lora_layer.lora_As[index]\n",
    "    lora_layer.lora_B = lora_layer.lora_Bs[index]\n",
    "\n",
    "\n",
    "def select_lora(model, index):\n",
    "    model.apply(apply_to_lora(lambda x: _select_lora(x, index)))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03fd1a90",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-01T08:35:52.569028Z",
     "start_time": "2024-07-01T08:35:52.565776Z"
    }
   },
   "source": [
    "## simle model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a6c8aab7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T07:04:15.641088Z",
     "start_time": "2024-07-02T07:04:15.637099Z"
    }
   },
   "outputs": [],
   "source": [
    "_ = torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "2f49596e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T07:56:57.057088Z",
     "start_time": "2024-07-02T07:56:57.052709Z"
    }
   },
   "outputs": [],
   "source": [
    "# a simple model\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(in_features=5, out_features=7),\n",
    "    torch.nn.Linear(in_features=7, out_features=3),\n",
    ")\n",
    "x = torch.randn(1,5)\n",
    "y = model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "e0e13956",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T07:56:57.273600Z",
     "start_time": "2024-07-02T07:56:57.267823Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.4970,  0.2374, -1.5260, -2.1606, -1.0945]])\n",
      "tensor([[0.3338, 0.0372, 0.1265]])\n"
     ]
    }
   ],
   "source": [
    "print(x)\n",
    "print(y)\n",
    "y0 = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "d0a7ab6d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T07:56:57.488567Z",
     "start_time": "2024-07-02T07:56:57.482572Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.nn.modules.container.Sequential"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "41b09f4a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T07:56:57.914925Z",
     "start_time": "2024-07-02T07:56:57.910375Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=5, out_features=7, bias=True)\n",
      "  (1): Linear(in_features=7, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "af5aac44",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T07:56:58.172303Z",
     "start_time": "2024-07-02T07:56:58.164233Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('0.weight',\n",
       "              tensor([[-0.4462,  0.3822,  0.1326, -0.3324, -0.1871],\n",
       "                      [-0.2870,  0.3693, -0.2978, -0.3866, -0.1442],\n",
       "                      [-0.2644,  0.4316,  0.1074, -0.4259,  0.2431],\n",
       "                      [ 0.1036, -0.2520, -0.0315,  0.2129,  0.3285],\n",
       "                      [ 0.3182,  0.1956, -0.4419, -0.3272, -0.2717],\n",
       "                      [-0.4340,  0.4016, -0.2010, -0.1053,  0.2090],\n",
       "                      [-0.2288, -0.0553,  0.1590, -0.4251, -0.3897]])),\n",
       "             ('0.bias',\n",
       "              tensor([ 0.3816, -0.2553,  0.2329,  0.1657, -0.1987, -0.1720,  0.2264])),\n",
       "             ('1.weight',\n",
       "              tensor([[-0.1907, -0.2413, -0.1805, -0.1045,  0.3615,  0.3016,  0.3322],\n",
       "                      [ 0.3732, -0.0422,  0.0975,  0.3448,  0.0249,  0.1269, -0.1858],\n",
       "                      [-0.3176, -0.0966,  0.0756, -0.0021, -0.0807, -0.0337,  0.3398]])),\n",
       "             ('1.bias', tensor([-0.1578,  0.0465,  0.2190]))])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "7caeb944",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T07:56:58.363800Z",
     "start_time": "2024-07-02T07:56:58.360370Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer name: , Layer type: <class 'torch.nn.modules.container.Sequential'>\n",
      "Layer name: 0, Layer type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Layer name: 1, Layer type: <class 'torch.nn.modules.linear.Linear'>\n"
     ]
    }
   ],
   "source": [
    "for name, layer in model.named_modules():\n",
    "    print(f\"Layer name: {name}, Layer type: {type(layer)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ce3c47",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-01T09:11:34.462678Z",
     "start_time": "2024-07-01T09:11:34.459515Z"
    }
   },
   "source": [
    "## add lora model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "7cbb0391",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T07:56:58.997397Z",
     "start_time": "2024-07-02T07:56:58.991920Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lora_config:  {<class 'torch.nn.modules.linear.Linear'>: {'weight': functools.partial(<bound method LoRAParametrization.from_linear of <class '__main__.LoRAParametrization'>>, rank=3)}}\n",
      "Original layer:  <class 'torch.nn.modules.linear.Linear'>\n",
      "LoRA Layer:  <class 'torch.nn.utils.parametrize.ParametrizedLinear'>\n",
      "--------------------\n",
      "Original layer:  <class 'torch.nn.modules.linear.Linear'>\n",
      "LoRA Layer:  <class 'torch.nn.utils.parametrize.ParametrizedLinear'>\n",
      "--------------------\n",
      "Original layer:  <class 'torch.nn.modules.container.Sequential'>\n"
     ]
    }
   ],
   "source": [
    "# add lora to the model\n",
    "# because B is initialized to 0, the output is the same as before\n",
    "add_lora(model)\n",
    "y = model(x)\n",
    "assert torch.allclose(y, y0)  # 用于比较两个tensor是否近似相等的函数，相近返回true，否则是false。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "afef6329",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T07:56:59.238237Z",
     "start_time": "2024-07-02T07:56:59.232730Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): ParametrizedLinear(\n",
      "    in_features=5, out_features=7, bias=True\n",
      "    (parametrizations): ModuleDict(\n",
      "      (weight): ParametrizationList(\n",
      "        (0): LoRAParametrization()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (1): ParametrizedLinear(\n",
      "    in_features=7, out_features=3, bias=True\n",
      "    (parametrizations): ModuleDict(\n",
      "      (weight): ParametrizationList(\n",
      "        (0): LoRAParametrization()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "1f4ae007",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T07:56:59.431285Z",
     "start_time": "2024-07-02T07:56:59.422039Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('0.bias',\n",
       "              tensor([ 0.3816, -0.2553,  0.2329,  0.1657, -0.1987, -0.1720,  0.2264])),\n",
       "             ('0.parametrizations.weight.original',\n",
       "              tensor([[-0.4462,  0.3822,  0.1326, -0.3324, -0.1871],\n",
       "                      [-0.2870,  0.3693, -0.2978, -0.3866, -0.1442],\n",
       "                      [-0.2644,  0.4316,  0.1074, -0.4259,  0.2431],\n",
       "                      [ 0.1036, -0.2520, -0.0315,  0.2129,  0.3285],\n",
       "                      [ 0.3182,  0.1956, -0.4419, -0.3272, -0.2717],\n",
       "                      [-0.4340,  0.4016, -0.2010, -0.1053,  0.2090],\n",
       "                      [-0.2288, -0.0553,  0.1590, -0.4251, -0.3897]])),\n",
       "             ('0.parametrizations.weight.0.lora_A',\n",
       "              tensor([[ 0.0139, -0.2692,  0.2622, -0.1078,  0.1925],\n",
       "                      [ 0.2196,  0.3292,  0.3107, -0.2908, -0.0264],\n",
       "                      [ 0.1487, -0.2220,  0.0665, -0.0764,  0.3098]])),\n",
       "             ('0.parametrizations.weight.0.lora_B',\n",
       "              tensor([[0., 0., 0.],\n",
       "                      [0., 0., 0.],\n",
       "                      [0., 0., 0.],\n",
       "                      [0., 0., 0.],\n",
       "                      [0., 0., 0.],\n",
       "                      [0., 0., 0.],\n",
       "                      [0., 0., 0.]])),\n",
       "             ('0.parametrizations.weight.0.lora_dropout_mask',\n",
       "              tensor([[1., 1., 1., 1., 1.]])),\n",
       "             ('1.bias', tensor([-0.1578,  0.0465,  0.2190])),\n",
       "             ('1.parametrizations.weight.original',\n",
       "              tensor([[-0.1907, -0.2413, -0.1805, -0.1045,  0.3615,  0.3016,  0.3322],\n",
       "                      [ 0.3732, -0.0422,  0.0975,  0.3448,  0.0249,  0.1269, -0.1858],\n",
       "                      [-0.3176, -0.0966,  0.0756, -0.0021, -0.0807, -0.0337,  0.3398]])),\n",
       "             ('1.parametrizations.weight.0.lora_A',\n",
       "              tensor([[-0.3556, -0.0280, -0.1609, -0.1322,  0.2283,  0.3501, -0.3578],\n",
       "                      [ 0.2434, -0.2893, -0.0074,  0.1511,  0.1449, -0.1915,  0.0479],\n",
       "                      [-0.2694, -0.2712,  0.1439,  0.3593,  0.3435, -0.3106,  0.2793]])),\n",
       "             ('1.parametrizations.weight.0.lora_B',\n",
       "              tensor([[0., 0., 0.],\n",
       "                      [0., 0., 0.],\n",
       "                      [0., 0., 0.]])),\n",
       "             ('1.parametrizations.weight.0.lora_dropout_mask',\n",
       "              tensor([[1., 1., 1., 1., 1., 1., 1.]]))])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "ee2cfded",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T07:56:59.861571Z",
     "start_time": "2024-07-02T07:56:59.857486Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer name: , Layer type: <class 'torch.nn.modules.container.Sequential'>\n",
      "Layer name: 0, Layer type: <class 'torch.nn.utils.parametrize.ParametrizedLinear'>\n",
      "Layer name: 0.parametrizations, Layer type: <class 'torch.nn.modules.container.ModuleDict'>\n",
      "Layer name: 0.parametrizations.weight, Layer type: <class 'torch.nn.utils.parametrize.ParametrizationList'>\n",
      "Layer name: 0.parametrizations.weight.0, Layer type: <class '__main__.LoRAParametrization'>\n",
      "Layer name: 1, Layer type: <class 'torch.nn.utils.parametrize.ParametrizedLinear'>\n",
      "Layer name: 1.parametrizations, Layer type: <class 'torch.nn.modules.container.ModuleDict'>\n",
      "Layer name: 1.parametrizations.weight, Layer type: <class 'torch.nn.utils.parametrize.ParametrizationList'>\n",
      "Layer name: 1.parametrizations.weight.0, Layer type: <class '__main__.LoRAParametrization'>\n"
     ]
    }
   ],
   "source": [
    "for name, layer in model.named_modules():\n",
    "    print(f\"Layer name: {name}, Layer type: {type(layer)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34fed363",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T07:03:41.282089Z",
     "start_time": "2024-07-02T07:03:41.278347Z"
    }
   },
   "source": [
    "### initialize lora_B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "b7cf4d59",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T07:57:00.495948Z",
     "start_time": "2024-07-02T07:57:00.490446Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.2669, -0.0787,  0.1015]])\n"
     ]
    }
   ],
   "source": [
    "# to make the output different, we need to initialize B to something non-zero\n",
    "model.apply(apply_to_lora(lambda x: torch.nn.init.ones_(x.lora_B)))\n",
    "y = model(x)\n",
    "print(y)\n",
    "assert not torch.allclose(y, y0)  # 没有返回表示true，即y ≠ y0\n",
    "y1 = y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b669cf02",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T07:02:58.609847Z",
     "start_time": "2024-07-02T07:02:58.606682Z"
    }
   },
   "source": [
    "### disable lora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "c5ab2e22",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T07:57:00.852731Z",
     "start_time": "2024-07-02T07:57:00.848741Z"
    }
   },
   "outputs": [],
   "source": [
    "# now let's try to disable lora, the output is the same as before lora is added.\n",
    "disable_lora(model)\n",
    "y = model(x)\n",
    "assert torch.allclose(y, y0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "dd1c9e3f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T07:57:01.246668Z",
     "start_time": "2024-07-02T07:57:01.240553Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.4970,  0.2374, -1.5260, -2.1606, -1.0945]])"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "1e6fa465",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T07:57:01.461496Z",
     "start_time": "2024-07-02T07:57:01.455963Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3338, 0.0372, 0.1265]])"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "e7903c3a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T07:57:01.644186Z",
     "start_time": "2024-07-02T07:57:01.639549Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): ParametrizedLinear(\n",
      "    in_features=5, out_features=7, bias=True\n",
      "    (parametrizations): ModuleDict(\n",
      "      (weight): ParametrizationList(\n",
      "        (0): LoRAParametrization()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (1): ParametrizedLinear(\n",
      "    in_features=7, out_features=3, bias=True\n",
      "    (parametrizations): ModuleDict(\n",
      "      (weight): ParametrizationList(\n",
      "        (0): LoRAParametrization()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "ee6884f2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T07:57:02.347485Z",
     "start_time": "2024-07-02T07:57:02.343956Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer name: , Layer type: <class 'torch.nn.modules.container.Sequential'>\n",
      "Layer name: 0, Layer type: <class 'torch.nn.utils.parametrize.ParametrizedLinear'>\n",
      "Layer name: 0.parametrizations, Layer type: <class 'torch.nn.modules.container.ModuleDict'>\n",
      "Layer name: 0.parametrizations.weight, Layer type: <class 'torch.nn.utils.parametrize.ParametrizationList'>\n",
      "Layer name: 0.parametrizations.weight.0, Layer type: <class '__main__.LoRAParametrization'>\n",
      "Layer name: 1, Layer type: <class 'torch.nn.utils.parametrize.ParametrizedLinear'>\n",
      "Layer name: 1.parametrizations, Layer type: <class 'torch.nn.modules.container.ModuleDict'>\n",
      "Layer name: 1.parametrizations.weight, Layer type: <class 'torch.nn.utils.parametrize.ParametrizationList'>\n",
      "Layer name: 1.parametrizations.weight.0, Layer type: <class '__main__.LoRAParametrization'>\n"
     ]
    }
   ],
   "source": [
    "for name, layer in model.named_modules():\n",
    "    print(f\"Layer name: {name}, Layer type: {type(layer)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7351d507",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T07:03:06.086362Z",
     "start_time": "2024-07-02T07:03:06.083202Z"
    }
   },
   "source": [
    "### enable lora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "d84d4eab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T07:57:03.838503Z",
     "start_time": "2024-07-02T07:57:03.833516Z"
    }
   },
   "outputs": [],
   "source": [
    "# enable lora again\n",
    "enable_lora(model)\n",
    "y = model(x)\n",
    "assert torch.allclose(y, y1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b10207",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T07:30:59.012431Z",
     "start_time": "2024-07-02T07:30:59.008444Z"
    }
   },
   "source": [
    "### save lora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "02f1dfd7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T07:57:04.984697Z",
     "start_time": "2024-07-02T07:57:04.978713Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.bias\n",
      "0.parametrizations.weight.original\n",
      "0.parametrizations.weight.0.lora_A\n",
      "0.parametrizations.weight.0.lora_B\n",
      "0.parametrizations.weight.0.lora_dropout_mask\n",
      "1.bias\n",
      "1.parametrizations.weight.original\n",
      "1.parametrizations.weight.0.lora_A\n",
      "1.parametrizations.weight.0.lora_B\n",
      "1.parametrizations.weight.0.lora_dropout_mask\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['0.parametrizations.weight.0.lora_A', '0.parametrizations.weight.0.lora_B', '1.parametrizations.weight.0.lora_A', '1.parametrizations.weight.0.lora_B'])"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's save the state dict for later use\n",
    "state_dict_to_save = get_lora_state_dict(model)\n",
    "state_dict_to_save.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "0d24ba26",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T07:57:05.569252Z",
     "start_time": "2024-07-02T07:57:05.564265Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): ParametrizedLinear(\n",
      "    in_features=5, out_features=7, bias=True\n",
      "    (parametrizations): ModuleDict(\n",
      "      (weight): ParametrizationList(\n",
      "        (0): LoRAParametrization()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (1): ParametrizedLinear(\n",
      "    in_features=7, out_features=3, bias=True\n",
      "    (parametrizations): ModuleDict(\n",
      "      (weight): ParametrizationList(\n",
      "        (0): LoRAParametrization()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf40382",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T07:43:14.745387Z",
     "start_time": "2024-07-02T07:43:14.741609Z"
    }
   },
   "source": [
    "### remove lora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "73b70805",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T07:57:06.556822Z",
     "start_time": "2024-07-02T07:57:06.552213Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original layer:  <class '__main__.LoRAParametrization'>\n",
      "Original layer:  <class 'torch.nn.utils.parametrize.ParametrizationList'>\n",
      "Original layer:  <class 'torch.nn.modules.container.ModuleDict'>\n",
      "Original layer:  <class 'torch.nn.utils.parametrize.ParametrizedLinear'>\n",
      "Original layer:  <class '__main__.LoRAParametrization'>\n",
      "Original layer:  <class 'torch.nn.utils.parametrize.ParametrizationList'>\n",
      "Original layer:  <class 'torch.nn.modules.container.ModuleDict'>\n",
      "Original layer:  <class 'torch.nn.utils.parametrize.ParametrizedLinear'>\n",
      "Original layer:  <class 'torch.nn.modules.container.Sequential'>\n"
     ]
    }
   ],
   "source": [
    "remove_lora(model)  # remove 'parametrizations'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "3362a1b1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T07:57:07.439612Z",
     "start_time": "2024-07-02T07:57:07.434751Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.bias\n",
      "0.weight\n",
      "1.bias\n",
      "1.weight\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_keys([])"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict_of_model = get_lora_state_dict(model)\n",
    "state_dict_of_model.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "23de48f2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T07:57:08.377475Z",
     "start_time": "2024-07-02T07:57:08.373711Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=5, out_features=7, bias=True)\n",
      "  (1): Linear(in_features=7, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbde28f2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T07:54:16.810886Z",
     "start_time": "2024-07-02T07:54:16.807214Z"
    }
   },
   "source": [
    "### load lora back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "c1a8c61a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T07:57:10.567371Z",
     "start_time": "2024-07-02T07:57:10.559522Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lora_config:  {<class 'torch.nn.modules.linear.Linear'>: {'weight': functools.partial(<bound method LoRAParametrization.from_linear of <class '__main__.LoRAParametrization'>>, rank=3)}}\n",
      "Original layer:  <class 'torch.nn.modules.linear.Linear'>\n",
      "LoRA Layer:  <class 'torch.nn.utils.parametrize.ParametrizedLinear'>\n",
      "--------------------\n",
      "Original layer:  <class 'torch.nn.modules.linear.Linear'>\n",
      "LoRA Layer:  <class 'torch.nn.utils.parametrize.ParametrizedLinear'>\n",
      "--------------------\n",
      "Original layer:  <class 'torch.nn.modules.container.Sequential'>\n"
     ]
    }
   ],
   "source": [
    "# let's try to load the lora back\n",
    "# first we need to add lora to the model\n",
    "add_lora(model)\n",
    "# then we can load the parameters\n",
    "# strict=False is needed because we are loading a subset of the parameters\n",
    "_ = model.load_state_dict(state_dict_to_save, strict=False)\n",
    "y = model(x)\n",
    "assert torch.allclose(y, y1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb52457",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T07:57:23.642993Z",
     "start_time": "2024-07-02T07:57:23.639003Z"
    }
   },
   "source": [
    "### merge lora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "99854020",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T07:58:49.913253Z",
     "start_time": "2024-07-02T07:58:49.907307Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original layer:  <class '__main__.LoRAParametrization'>\n",
      "Original layer:  <class 'torch.nn.utils.parametrize.ParametrizationList'>\n",
      "Original layer:  <class 'torch.nn.modules.container.ModuleDict'>\n",
      "Original layer:  <class 'torch.nn.utils.parametrize.ParametrizedLinear'>\n",
      "Original layer:  <class '__main__.LoRAParametrization'>\n",
      "Original layer:  <class 'torch.nn.utils.parametrize.ParametrizationList'>\n",
      "Original layer:  <class 'torch.nn.modules.container.ModuleDict'>\n",
      "Original layer:  <class 'torch.nn.utils.parametrize.ParametrizedLinear'>\n",
      "Original layer:  <class 'torch.nn.modules.container.Sequential'>\n"
     ]
    }
   ],
   "source": [
    "# we can merge it to make it a normal linear layer, so there is no overhead for inference.\n",
    "merge_lora(model)\n",
    "y = model(x)\n",
    "assert torch.allclose(y, y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "372fd1b6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T07:59:43.174254Z",
     "start_time": "2024-07-02T07:59:43.170911Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=5, out_features=7, bias=True)\n",
      "  (1): Linear(in_features=7, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b6a983",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T08:00:19.756983Z",
     "start_time": "2024-07-02T08:00:19.753089Z"
    }
   },
   "source": [
    "## Training a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "11202c4a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T09:38:53.280638Z",
     "start_time": "2024-07-02T09:38:53.271209Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=5, out_features=3, bias=True)\n",
      "lora_config:  {<class 'torch.nn.modules.linear.Linear'>: {'weight': functools.partial(<bound method LoRAParametrization.from_linear of <class '__main__.LoRAParametrization'>>, rank=3)}}\n",
      "Original layer:  <class 'torch.nn.modules.linear.Linear'>\n",
      "LoRA Layer:  <class 'torch.nn.utils.parametrize.ParametrizedLinear'>\n",
      "--------------------\n",
      "ParametrizedLinear(\n",
      "  in_features=5, out_features=3, bias=True\n",
      "  (parametrizations): ModuleDict(\n",
      "    (weight): ParametrizationList(\n",
      "      (0): LoRAParametrization()\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = torch.nn.Linear(in_features=5, out_features=3)\n",
    "print(model)\n",
    "# step 1: Add LoRA to the model\n",
    "add_lora(model)\n",
    "print(model)\n",
    "\n",
    "# step 2: Collect the parameters, pass them to the optimizer\n",
    "parameters = [\n",
    "    {'params': list(get_lora_params(model))}\n",
    "]\n",
    "optimizer = torch.optim.AdamW(parameters, lr=1e-3)\n",
    "\n",
    "# step 3: Train the model\n",
    "# ...\n",
    "# Simulate training, update the LoRA parameters\n",
    "model.apply(apply_to_lora(lambda x: torch.nn.init.normal_(x.lora_A)))\n",
    "model.apply(apply_to_lora(lambda x: torch.nn.init.normal_(x.lora_B)))\n",
    "\n",
    "# step 4: export the LoRA parameters\n",
    "state_dict = model.state_dict()\n",
    "lora_state_dict = {k: v for k,v in state_dict.items() if name_is_lora(k)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "406a83cb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T09:38:56.532397Z",
     "start_time": "2024-07-02T09:38:56.528601Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ParametrizedLinear(\n",
      "  in_features=5, out_features=3, bias=True\n",
      "  (parametrizations): ModuleDict(\n",
      "    (weight): ParametrizationList(\n",
      "      (0): LoRAParametrization()\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da78826",
   "metadata": {},
   "source": [
    "## Loading and Inferencing with LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "99dc7db6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T09:39:00.047875Z",
     "start_time": "2024-07-02T09:39:00.042602Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lora_config:  {<class 'torch.nn.modules.linear.Linear'>: {'weight': functools.partial(<bound method LoRAParametrization.from_linear of <class '__main__.LoRAParametrization'>>, rank=3)}}\n",
      "Original layer:  <class '__main__.LoRAParametrization'>\n",
      "Original layer:  <class 'torch.nn.utils.parametrize.ParametrizationList'>\n",
      "Original layer:  <class 'torch.nn.modules.container.ModuleDict'>\n",
      "Original layer:  <class 'torch.nn.utils.parametrize.ParametrizedLinear'>\n",
      "Original layer:  <class '__main__.LoRAParametrization'>\n",
      "Original layer:  <class 'torch.nn.utils.parametrize.ParametrizationList'>\n",
      "Original layer:  <class 'torch.nn.modules.container.ModuleDict'>\n",
      "Original layer:  <class 'torch.nn.utils.parametrize.ParametrizedLinear'>\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Add LoRA to your model\n",
    "add_lora(model)\n",
    "\n",
    "# Step 2: Load the LoRA parameters\n",
    "_ = model.load_state_dict(lora_state_dict, strict=False)\n",
    "\n",
    "# Step 3: Merge the LoRA parameters into the model\n",
    "merge_lora(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "b6104908",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T09:39:04.049097Z",
     "start_time": "2024-07-02T09:39:04.044109Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=5, out_features=3, bias=True)\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911bb768",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T08:54:18.747321Z",
     "start_time": "2024-07-02T08:54:18.744649Z"
    }
   },
   "source": [
    "## Inferencign with multiple LoRA models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "59b19575",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T09:39:09.108216Z",
     "start_time": "2024-07-02T09:39:09.101234Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original layer:  <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=5, out_features=3, bias=True)\n",
      "lora_config:  {<class 'torch.nn.modules.linear.Linear'>: {'weight': functools.partial(<bound method LoRAParametrization.from_linear of <class '__main__.LoRAParametrization'>>, rank=3)}}\n",
      "Original layer:  <class 'torch.nn.modules.linear.Linear'>\n",
      "LoRA Layer:  <class 'torch.nn.utils.parametrize.ParametrizedLinear'>\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "# to avoid re-adding LoRA to the model when return the cell, remove lora first\n",
    "remove_lora(model)\n",
    "print(model)\n",
    "# Step 1: Add lora to your model\n",
    "add_lora(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "2cc7682d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T09:39:26.504992Z",
     "start_time": "2024-07-02T09:39:26.499802Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ParametrizedLinear(\n",
      "  in_features=5, out_features=3, bias=True\n",
      "  (parametrizations): ModuleDict(\n",
      "    (weight): ParametrizationList(\n",
      "      (0): LoRAParametrization()\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Load the lora parameters\n",
    "# fake 3 sets of LoRA parameters\n",
    "lora_state_dict_0 = lora_state_dict\n",
    "lora_state_dict_1 = {k: torch.ones_like(v) for k,v in lora_state_dict.items()}\n",
    "lora_state_dict_2 = {k: torch.zeros_like(v) for k,v in lora_state_dict.items()}\n",
    "lora_state_dicts = [lora_state_dict_0, lora_state_dict_1, lora_state_dict_2]\n",
    "\n",
    "# 加载多套参数state_dict\n",
    "load_multiple_lora(model, lora_state_dicts)\n",
    "\n",
    "# 检查参数\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "629e032e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T09:39:32.478125Z",
     "start_time": "2024-07-02T09:39:32.470673Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'parametrizations.weight.0.lora_A': tensor([[-0.6389,  1.1025, -0.4028,  0.1417,  1.2188],\n",
       "          [ 1.2653,  0.3596, -0.5528, -0.4428,  0.0385],\n",
       "          [-1.1018, -1.9272, -1.0012, -0.9899,  2.2904]]),\n",
       "  'parametrizations.weight.0.lora_B': tensor([[ 1.0520,  0.5619,  1.2419],\n",
       "          [ 0.0190, -0.1718, -1.1967],\n",
       "          [-1.5829, -2.0740, -0.3178]])},\n",
       " {'parametrizations.weight.0.lora_A': tensor([[1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1.]]),\n",
       "  'parametrizations.weight.0.lora_B': tensor([[1., 1., 1.],\n",
       "          [1., 1., 1.],\n",
       "          [1., 1., 1.]])},\n",
       " {'parametrizations.weight.0.lora_A': tensor([[0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.]]),\n",
       "  'parametrizations.weight.0.lora_B': tensor([[0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.]])}]"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lora_state_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "278e36ad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T09:39:42.323754Z",
     "start_time": "2024-07-02T09:39:42.318736Z"
    }
   },
   "outputs": [],
   "source": [
    "# step 3: Select which LoRA to use at inference time\n",
    "Y0 = select_lora(model, 0)(x)  # 令lora_layer.lora_A = index指定的参数\n",
    "Y1 = select_lora(model, 1)(x)\n",
    "Y2 = select_lora(model, 2)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "04e2b17d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T09:41:33.355447Z",
     "start_time": "2024-07-02T09:41:33.349044Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.0684, -1.2928, -2.5007]]),\n",
       " tensor([[-4.2630, -5.1295, -5.3923]]),\n",
       " tensor([[-0.2162, -1.0827, -1.3455]]))"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y0, Y1, Y2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cbfebed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T09:42:59.220563Z",
     "start_time": "2024-07-02T09:42:59.217432Z"
    }
   },
   "source": [
    "### merge lora "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "f6cd91c7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T09:43:56.934205Z",
     "start_time": "2024-07-02T09:43:56.928024Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original layer:  <class '__main__.LoRAParametrization'>\n",
      "Original layer:  <class 'torch.nn.utils.parametrize.ParametrizationList'>\n",
      "Original layer:  <class 'torch.nn.modules.container.ModuleDict'>\n",
      "Original layer:  <class 'torch.nn.utils.parametrize.ParametrizedLinear'>\n",
      "Linear(in_features=5, out_features=3, bias=True)\n",
      "OrderedDict([('bias', tensor([-0.2535, -0.3261,  0.3318])), ('weight', tensor([[-0.3439,  0.0394, -0.7497, -0.0762,  1.0139],\n",
      "        [-0.0388,  0.7641,  0.7504,  0.2345, -0.6696],\n",
      "        [-0.3932, -0.8996,  1.1107,  0.1917, -0.7682]]))])\n"
     ]
    }
   ],
   "source": [
    "remove_lora(model)\n",
    "init_state_dict = model.state_dict()\n",
    "print(model)\n",
    "print(init_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "2399fdfc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T09:46:08.064686Z",
     "start_time": "2024-07-02T09:46:08.056050Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original layer:  <class 'torch.nn.modules.linear.Linear'>\n",
      "lora_config:  {<class 'torch.nn.modules.linear.Linear'>: {'weight': functools.partial(<bound method LoRAParametrization.from_linear of <class '__main__.LoRAParametrization'>>, rank=3)}}\n",
      "Original layer:  <class 'torch.nn.modules.linear.Linear'>\n",
      "LoRA Layer:  <class 'torch.nn.utils.parametrize.ParametrizedLinear'>\n",
      "--------------------\n",
      "Original layer:  <class '__main__.LoRAParametrization'>\n",
      "Original layer:  <class 'torch.nn.utils.parametrize.ParametrizationList'>\n",
      "Original layer:  <class 'torch.nn.modules.container.ModuleDict'>\n",
      "Original layer:  <class 'torch.nn.utils.parametrize.ParametrizedLinear'>\n",
      "tensor([[-0.0684, -1.2928, -2.5007]])\n",
      "Original layer:  <class 'torch.nn.modules.linear.Linear'>\n",
      "lora_config:  {<class 'torch.nn.modules.linear.Linear'>: {'weight': functools.partial(<bound method LoRAParametrization.from_linear of <class '__main__.LoRAParametrization'>>, rank=3)}}\n",
      "Original layer:  <class 'torch.nn.modules.linear.Linear'>\n",
      "LoRA Layer:  <class 'torch.nn.utils.parametrize.ParametrizedLinear'>\n",
      "--------------------\n",
      "Original layer:  <class '__main__.LoRAParametrization'>\n",
      "Original layer:  <class 'torch.nn.utils.parametrize.ParametrizationList'>\n",
      "Original layer:  <class 'torch.nn.modules.container.ModuleDict'>\n",
      "Original layer:  <class 'torch.nn.utils.parametrize.ParametrizedLinear'>\n",
      "tensor([[-4.2630, -5.1295, -5.3923]])\n",
      "Original layer:  <class 'torch.nn.modules.linear.Linear'>\n",
      "lora_config:  {<class 'torch.nn.modules.linear.Linear'>: {'weight': functools.partial(<bound method LoRAParametrization.from_linear of <class '__main__.LoRAParametrization'>>, rank=3)}}\n",
      "Original layer:  <class 'torch.nn.modules.linear.Linear'>\n",
      "LoRA Layer:  <class 'torch.nn.utils.parametrize.ParametrizedLinear'>\n",
      "--------------------\n",
      "Original layer:  <class '__main__.LoRAParametrization'>\n",
      "Original layer:  <class 'torch.nn.utils.parametrize.ParametrizationList'>\n",
      "Original layer:  <class 'torch.nn.modules.container.ModuleDict'>\n",
      "Original layer:  <class 'torch.nn.utils.parametrize.ParametrizedLinear'>\n",
      "tensor([[-0.2162, -1.0827, -1.3455]])\n"
     ]
    }
   ],
   "source": [
    "# verify that it's the same as if we load the lora parameters one by one\n",
    "for state_dict in lora_state_dicts:\n",
    "    remove_lora(model)\n",
    "    _ = model.load_state_dict(init_state_dict, strict=False)\n",
    "    add_lora(model)\n",
    "    _ = model.load_state_dict(state_dict, strict=False)\n",
    "    merge_lora(model)\n",
    "    y = model(x)\n",
    "    print(y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
