{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd243367",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-04T09:07:05.066094Z",
     "start_time": "2024-07-04T09:07:05.061595Z"
    }
   },
   "outputs": [],
   "source": [
    "# Pytorch example: Finetuning Llama2 with LoRA\n",
    "# https://pytorch.org/torchtune/stable/tutorials/lora_finetune.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884fda3b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-04T08:23:44.404012Z",
     "start_time": "2024-07-04T08:23:44.400863Z"
    }
   },
   "source": [
    "## Minimal LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82e2306f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-04T09:07:08.987665Z",
     "start_time": "2024-07-04T09:07:05.930369Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch import Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b1f5d81",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-04T09:07:08.999147Z",
     "start_time": "2024-07-04T09:07:08.990167Z"
    }
   },
   "outputs": [],
   "source": [
    "class LoRALinear(nn.Module):\n",
    "    def __init__(self, in_dim:int, out_dim:int, rank:int, alpha:float, dropout:float):\n",
    "        # These are the weights from the original pretrained model\n",
    "        self.linear = nn.Linear(in_dim, out_dim, bias=False)\n",
    "        \n",
    "        # These are the new LoRA params, In general rank << in_dim & out_dim\n",
    "        self.lora_a = nn.Linear(in_dim, rank, bias=False)\n",
    "        self.lora_b = nn.Linear(rank, out_dim, bias=False)\n",
    "        \n",
    "        # Rank and alpha are commonly-tuned hyperparameters\n",
    "        self.rank = rank\n",
    "        self.alpha = alpha\n",
    "        \n",
    "        # Most implementations also include some dropout\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        # The original params are frozen, and only LoRA params are trainable\n",
    "        self.linear.weight.requires_grad = False\n",
    "        self.lora_a.weight.requires_grad = True\n",
    "        self.lora_b.weight.requires_grad = True\n",
    "    \n",
    "    def forward(self, x:Tensor) -> Tensor:\n",
    "        # This would be the output of the original model\n",
    "        frozen_out = self.linear(x)\n",
    "        \n",
    "        # lora_a projects inputs down to the much smaller self.rank,\n",
    "        # then lora_b projects back up to the output dimension\n",
    "        lora_out = self.lora_b(self.lora_a(self.dropout(x)))\n",
    "        \n",
    "        # Finally, scale by the alpha parameter (normalized by rank)\n",
    "        # and add to the original model's outputs\n",
    "        return frozen_out + (self.alpha / self.rank) * lora_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa1f907",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-04T08:46:54.206021Z",
     "start_time": "2024-07-04T08:46:54.202017Z"
    }
   },
   "source": [
    "## Applying LoRA to Llama2 models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a7641cd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-04T09:20:29.035879Z",
     "start_time": "2024-07-04T09:20:29.026446Z"
    }
   },
   "outputs": [],
   "source": [
    "# problem: 内存占用过大，jupyter内核挂掉了\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eab1d0b6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-04T09:20:37.179655Z",
     "start_time": "2024-07-04T09:20:29.484039Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    torchtune, https://pytorch.org/torchtune/stable/index.html\\n    - Llama3 in torchtune\\n    - Finetuning with LoRA in torchtune\\n    - Understanding QLoRA in TorchTune\\n    - End-to-End Workflow with torchtune\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchtune.models.llama2 import llama2_7b, lora_llama2_7b  # torchtune是一个torch库，用于轻松创作、微调和试验LLM。\n",
    "'''\n",
    "    torchtune, https://pytorch.org/torchtune/stable/index.html\n",
    "    - Llama3 in torchtune\n",
    "    - Finetuning with LoRA in torchtune\n",
    "    - Understanding QLoRA in TorchTune\n",
    "    - End-to-End Workflow with torchtune\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb303731",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-04T09:19:07.556663Z",
     "start_time": "2024-07-04T09:19:07.553920Z"
    }
   },
   "source": [
    "### usual model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1662031c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-04T09:17:16.988380Z",
     "start_time": "2024-07-04T09:17:08.889726Z"
    }
   },
   "outputs": [],
   "source": [
    "# Build Llama2 without any LoRA layers\n",
    "base_model = llama2_7b()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d53f55a4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-04T09:17:44.227326Z",
     "start_time": "2024-07-04T09:17:44.221704Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CausalSelfAttention(\n",
       "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "  (output_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "  (pos_embeddings): RotaryPositionalEmbeddings()\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print the first layer's self-attention in the usual Llama2 model\n",
    "base_model.layers[0].attn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92863f3d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-04T09:18:51.733659Z",
     "start_time": "2024-07-04T09:18:51.727899Z"
    }
   },
   "source": [
    "### lora model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8993f4ce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-04T09:20:49.065958Z",
     "start_time": "2024-07-04T09:20:40.962667Z"
    }
   },
   "outputs": [],
   "source": [
    "# The default settings for lora_llama2_7b will match those for llama2_7b\n",
    "# We just need to define which layers we want LoRA applied to\n",
    "# Within each self-attention, we can choose from ['q_proj', 'k_proj', 'v_proj', 'output_proj']\n",
    "# We can also set apply_lora_to_mlp=True or apply_lora_to_output=True to apply LoRA to other linear\n",
    "# layers outside of the self-attention\n",
    "lora_model = lora_llama2_7b(lora_attn_modules=['q_proj', 'v_proj'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37076c1c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-04T09:20:52.210266Z",
     "start_time": "2024-07-04T09:20:52.204283Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CausalSelfAttention(\n",
       "  (q_proj): LoRALinear(\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "    (lora_a): Linear(in_features=4096, out_features=8, bias=False)\n",
       "    (lora_b): Linear(in_features=8, out_features=4096, bias=False)\n",
       "  )\n",
       "  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "  (v_proj): LoRALinear(\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "    (lora_a): Linear(in_features=4096, out_features=8, bias=False)\n",
       "    (lora_b): Linear(in_features=8, out_features=4096, bias=False)\n",
       "  )\n",
       "  (output_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "  (pos_embeddings): RotaryPositionalEmbeddings()\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print the same for Llama2 with LoRA weights\n",
    "lora_model.layers[0].attn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9ab030",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-04T09:21:33.887398Z",
     "start_time": "2024-07-04T09:21:33.884427Z"
    }
   },
   "source": [
    "### lora load weights of base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf520c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming that base_model already has the pretrained Llama2 weights,\n",
    "# this will directly load them into your LoRA model without any conversion necessary.\n",
    "lora_model.load_state_dict(base_model.state_dict(), strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44e9f799",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-04T09:23:57.387096Z",
     "start_time": "2024-07-04T09:23:57.383577Z"
    }
   },
   "outputs": [],
   "source": [
    "from torchtune.modules.peft.peft_utils import get_adapter_params, set_trainable_params\n",
    "# get_adapter_params, 返回model中对应于adapter的参数子集\n",
    "# set_trainable_params, 根据adapter参数的状态字典设置nn.Module的可训练参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "712499c1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-04T09:39:35.384278Z",
     "start_time": "2024-07-04T09:39:35.368130Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    6742609920 total params,\n",
      "    4194304\" trainable params,\n",
      "    0.06% of all params are trainable \n",
      "    \n"
     ]
    }
   ],
   "source": [
    "# Fetch all params from the model that are associated with LoRA\n",
    "lora_params = get_adapter_params(lora_model)\n",
    "\n",
    "# Set requires_grad=True on lora_params, and requires_grad=False on all others\n",
    "set_trainable_params(lora_model, lora_params)\n",
    "\n",
    "# Print the total number of parameters\n",
    "total_params = sum([p.numel() for p in lora_model.parameters()])\n",
    "trainable_params = sum([p.numel() for p in lora_model.parameters() if p.requires_grad])\n",
    "print(\n",
    "    f\"\"\"\n",
    "    {total_params} total params,\n",
    "    {trainable_params}\" trainable params,\n",
    "    {(100.0 * trainable_params / total_params):.2f}% of all params are trainable \n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35838aeb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-04T09:39:58.987731Z",
     "start_time": "2024-07-04T09:39:58.984165Z"
    }
   },
   "source": [
    "## LoRA finetuning recipe in torchtune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9726523f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tune run --nnodes 1 --nproc_per_node 2 lora_finetune_distributed --config llama2/7B_lora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7483b257",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Arguments\n",
    "model:\n",
    "  _component_: lora_llama2_7b\n",
    "  lora_attn_modules: ['q_proj', 'v_proj']\n",
    "  lora_rank: 8\n",
    "  lora_alpha: 16\n",
    "..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
