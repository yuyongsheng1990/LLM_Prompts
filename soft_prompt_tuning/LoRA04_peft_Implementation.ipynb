{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d4f50a7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-05T00:24:04.167163Z",
     "start_time": "2024-07-05T00:24:04.163359Z"
    }
   },
   "source": [
    "## peft quickstart demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ef6289",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-05T01:48:14.835726Z",
     "start_time": "2024-07-05T01:48:14.831736Z"
    }
   },
   "source": [
    "###  AutoModelForSeq2SeqLM introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "29ca170f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-05T01:50:59.468530Z",
     "start_time": "2024-07-05T01:50:41.293900Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3718190677c449928e5535ff92901cc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\yysgz\\.cache\\huggingface\\hub\\models--t5-small. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf51d370f8b5487a9c7e570162f42856",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/242M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee5cfb78f42e41dfbce099e31ba0fff1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ae919b2426c478ea88ca48f35d38017",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.32k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7536d8dc6812427e890f7ed2dfb88739",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33f1454cb6e645f5b8a64b1a1018052c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Translate English to French: How are you?\n",
      "Output: Comment êtes-vous?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\Lib\\site-packages\\transformers\\generation\\utils.py:1168: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "    # 指定模型名称或路径\n",
    "    model_name = \"t5-small\"\n",
    "\n",
    "    # 加载预训练模型和分词器\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    # 输入文本\n",
    "    input_text = \"Translate English to French: How are you?\"\n",
    "\n",
    "    # 编码文本--成模型可接受的输入格式\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "\n",
    "    # 生成输出\n",
    "    outputs = model.generate(**inputs)\n",
    "\n",
    "    # 解码输出文本\n",
    "    output_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    print(f\"Input: {input_text}\")\n",
    "    print(f\"Output: {output_text}\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82dad836",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-05T01:55:35.742465Z",
     "start_time": "2024-07-05T01:55:35.738926Z"
    }
   },
   "source": [
    "### peft introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "81bab89b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-05T02:14:56.477495Z",
     "start_time": "2024-07-05T02:14:56.472970Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CAUSAL_LM', 'FEATURE_EXTRACTION', 'QUESTION_ANS', 'SEQ_2_SEQ_LM', 'SEQ_CLS', 'TOKEN_CLS', 'capitalize', 'casefold', 'center', 'count', 'encode', 'endswith', 'expandtabs', 'find', 'format', 'format_map', 'index', 'isalnum', 'isalpha', 'isascii', 'isdecimal', 'isdigit', 'isidentifier', 'islower', 'isnumeric', 'isprintable', 'isspace', 'istitle', 'isupper', 'join', 'ljust', 'lower', 'lstrip', 'maketrans', 'partition', 'removeprefix', 'removesuffix', 'replace', 'rfind', 'rindex', 'rjust', 'rpartition', 'rsplit', 'rstrip', 'split', 'splitlines', 'startswith', 'strip', 'swapcase', 'title', 'translate', 'upper', 'zfill']\n"
     ]
    }
   ],
   "source": [
    "print([task for task in dir(TaskType) if not task.startswith(\"__\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d032201d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-05T02:38:35.057364Z",
     "start_time": "2024-07-05T02:38:35.049868Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\npeft (Parameter-Efficient Fine-Tuning) package introduction:\\n    Fine-tuning large pretrained models is often prohibitively costly due to their scale. \\n    PEFT methods enable efficient adaptation of large pretrained models to various downstream applications by only fine-tuning a \\nsmall number of (extra) model parameters instead of all the model's parameters. \\n    This significantly decreases the computational and storage costs. \\n    Recent state-of-the-art PEFT techniques achieve performance comparable to fully fine-tuned models.\\n    PEFT is integrated with Transformers for easy model training and inference, \\npeft简化了LLM-finetuning 模型配置和加载功能，特别是使用LoRA等技术。\\n    - LoraConfig，用于配置LoRA参数。\\n    - TaskType，用于定义任务类型, e.g. task_type = TaskType.TEXT_GENERATION\\n    - get_peft_config，用于获取peft配置\\n    - get_peft_model，用于获取pretrained peft模型。\\n\""
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "peft (Parameter-Efficient Fine-Tuning) package introduction:\n",
    "    Fine-tuning large pretrained models is often prohibitively costly due to their scale. \n",
    "    PEFT methods enable efficient adaptation of large pretrained models to various downstream applications by only fine-tuning a \n",
    "small number of (extra) model parameters instead of all the model's parameters. \n",
    "    This significantly decreases the computational and storage costs. \n",
    "    Recent state-of-the-art PEFT techniques achieve performance comparable to fully fine-tuned models.\n",
    "    PEFT is integrated with Transformers for easy model training and inference, \n",
    "peft简化了LLM-finetuning 模型配置和加载功能，特别是使用LoRA等技术。\n",
    "    - LoraConfig，用于配置LoRA参数。\n",
    "    - TaskType，用于定义任务类型, e.g. task_type = TaskType.TEXT_GENERATION\n",
    "    - get_peft_config，用于获取peft配置\n",
    "    - get_peft_model，用于获取pretrained peft模型。\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd1d311",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-05T02:45:44.134917Z",
     "start_time": "2024-07-05T02:45:44.131453Z"
    }
   },
   "source": [
    "#### get_deft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2b8954b5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-05T02:51:28.463131Z",
     "start_time": "2024-07-05T02:51:19.103887Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 2,359,296 || all params: 1,231,940,608 || trainable%: 0.1915\n",
      "None\n",
      "tensor([[   0, 6679, 1169,  360,  264, 1465,  259,  291,    1]])\n",
      "Comment allez-vous?\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "---------------peft 翻译模型----------------------------\n",
    "# 翻译模型bigscience/mt0-large: English -> French\n",
    "'''\n",
    "# prepare a model for training with a PEFT method such as LoRA by wrapping the base model and PEFT configuration with get_peft_model.\n",
    "# For the bigscience/mt0-large model, you are only training 0.19% of the parameters!\n",
    "from transformers import AutoModelForSeq2SeqLM  # 用于加载和处理pre-trained seq2seq模型，用于处理nlp任务\n",
    "from peft import get_peft_config, get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "# 加载预训练模型和分词器 \n",
    "model_name = 'bigscience/mt0-large'\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# 定义lora配置\n",
    "lora_config = LoraConfig(\n",
    "    task_type = TaskType.SEQ_2_SEQ_LM, \n",
    "    inference_mode=False, \n",
    "    r=8, \n",
    "    lora_alpha=32, \n",
    "    lora_dropout=0.1\n",
    ")\n",
    "\n",
    "# 获取peft model\n",
    "peft_model = get_peft_model(model, peft_config)\n",
    "print(peft_model.print_trainable_parameters())  # 输出peft mode可训练参数\n",
    "\n",
    "# 准备输入数据\n",
    "input_text = \"Translate English to French: How are you?\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "\n",
    "# 使用 PEFT 模型生成输出\n",
    "outputs = peft_model.generate(**inputs)\n",
    "output_text = tokenizer.decode(outputs[0], skip_special_tokens=True)  # 解码\n",
    "print(outputs)\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b958ad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-05T00:40:40.043134Z",
     "start_time": "2024-07-05T00:40:40.043134Z"
    }
   },
   "source": [
    "#### load_peft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "202bc8dc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-05T02:53:25.079256Z",
     "start_time": "2024-07-05T02:53:16.961194Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    2, 22763, 25978,     5, 12941,     7, 10088,  4176,     8,   317,\n",
      "             5, 20931, 14397,    11,     5,  1312,     9,     5, 12941,     4,\n",
      "         50118, 50118,  1121,    10,   739,  5749,     6,  9637,     5, 15039,\n",
      "             6, 14814, 10477,     6, 14814, 18833,     6,  6740,     6,     8,\n",
      "         27053,     4, 50118, 50118,  1121,    10,  2559,  5749,     6,  9637,\n",
      "             5,  8380,  1423,  1168,  2258,     6,  4696,     6,     8, 21857,\n",
      "             4, 50118, 50118]])\n",
      "Preheat the oven to 350 degrees and place the cookie dough in the center of the oven.\n",
      "\n",
      "In a large bowl, combine the flour, baking powder, baking soda, salt, and cinnamon.\n",
      "\n",
      "In a separate bowl, combine the egg yolks, sugar, and vanilla.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "-----------------peft 因果推理模型--------------------\n",
    "因果推理模型 ybelkada/opt-350m-lora; gpt2\n",
    "'''\n",
    "from peft import AutoPeftModelForCausalLM  # 用于加载和配置因果语言模型Causal LM，并进行高效微调参数\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = AutoPeftModelForCausalLM.from_pretrained('ybelkada/opt-350m-lora').to(device) \n",
    "tokenizer = AutoTokenizer.from_pretrained('facebook/opt-350m')\n",
    "\n",
    "model.eval()\n",
    "inputs = tokenizer('Preheat the oven to 350 degrees and place the cookie dough', return_tensors='pt')\n",
    "\n",
    "outputs = model.generate(input_ids=inputs['input_ids'].to(device), max_new_tokens=50)  # 生成输出\n",
    "outputs_text = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]  # tokenizer解码输出文本\n",
    "print(outputs)\n",
    "print(outputs_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04626a7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-05T00:17:19.320384Z",
     "start_time": "2024-07-05T00:17:19.316573Z"
    }
   },
   "source": [
    "## LoRA Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "929e605d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-05T06:59:09.438691Z",
     "start_time": "2024-07-05T06:59:09.419697Z"
    }
   },
   "outputs": [],
   "source": [
    "# peft package implement LoRA: https://github.com/hahuyhoang411/LoRA-Implementation/tree/main"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651e5bfd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-05T07:00:27.987197Z",
     "start_time": "2024-07-05T07:00:27.982553Z"
    }
   },
   "source": [
    "### prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3b46575a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-05T07:11:27.598653Z",
     "start_time": "2024-07-05T07:11:27.588682Z"
    }
   },
   "outputs": [],
   "source": [
    "from typing import Union\n",
    "\n",
    "class Prompter(object):\n",
    "    __slots__=('template')\n",
    "    \n",
    "    def __init__(self, template_name: str = '', verbose: bool = False):\n",
    "        self.template = {\n",
    "            'description': 'Template used by Alpaca-LoRA.',\n",
    "            'prompt_input': 'Below is an instruction that describes a task, paired with an input that provides further context. \\\n",
    "             Write a response taht appropriately completes the request.',\n",
    "            'prompt_no_input': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.',\n",
    "            'response_split': '### Response:'}\n",
    "    \n",
    "    def generate_prompt(self, instruction:str, input: Union[None, str]=None, label: Union[None, str]=None) -> str:\n",
    "        # returns the full prompt from instruction and optional input\n",
    "        # if a label (=response, =output) is provided, it's also appended.\n",
    "        if input:\n",
    "            res = self.template['promt_input'].format(instruction=instruction, input=input)\n",
    "        else:\n",
    "            res = self.template['prompt_no_input'].format(instruction=instruction)\n",
    "        if label:\n",
    "            res = f\"{res}{label}\"\n",
    "        return res\n",
    "    \n",
    "    def get_response(self, output: str) -> str:\n",
    "        return output.split(self.template['response_split'])[1].strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09927cb9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-05T06:59:29.185687Z",
     "start_time": "2024-07-05T06:59:29.181813Z"
    }
   },
   "source": [
    "### prepare_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "90ea9738",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-05T07:16:54.736876Z",
     "start_time": "2024-07-05T07:16:54.733571Z"
    }
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import random\n",
    "from typing import Union  # 类型提示，允许你指定的一个变量可以是多个类型中的任意一种"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a8d0271f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-05T07:25:01.158729Z",
     "start_time": "2024-07-05T07:25:01.152045Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_datasets(data_path, size_valid_set, tokenizer, max_length, seed):\n",
    "    def tokenize(prompt, add_eos_token=True):\n",
    "        result = tokenizer(\n",
    "            prompt,\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            padding=False,\n",
    "            return_tensors=None\n",
    "        )\n",
    "        \n",
    "        if (result['input_ids'][-1] != tokenizer.eos_token_id\n",
    "           and len(result['input_ids']) < max_length\n",
    "           and add_eos_token):\n",
    "            result['input_ids'].append(tokenizer.eos_token_id)\n",
    "            result['attention_mask'].append(1)\n",
    "        \n",
    "        result['labels'] = result['input_ids'].copy()\n",
    "        return result\n",
    "    \n",
    "    def generate_and_tokenize_prompt(data_point):\n",
    "        full_prompt = prompter.generate_prompt(\n",
    "            data_point['instruction'],\n",
    "            data_point['input'],\n",
    "            data_point['output'],\n",
    "        )\n",
    "        tokenized_full_prompt = tokenize(full_prompt)\n",
    "        \n",
    "        return tokenized_full_prompt\n",
    "    \n",
    "    prompter = Prompter()\n",
    "    \n",
    "    print(f'Load dataset...')\n",
    "    dataset = load_dataset('json', split='train', datafiles=data_path)\n",
    "    dataset = dataset.train_test_split(test_size=size_valid_set, seed=seed)\n",
    "    \n",
    "    train_data = dataset['train'].shuffle().map(generate_and_tokenize_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93764ebf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-05T07:25:19.223811Z",
     "start_time": "2024-07-05T07:25:19.221006Z"
    }
   },
   "source": [
    "### lora model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "fc49ed33",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-05T07:26:04.814172Z",
     "start_time": "2024-07-05T07:26:04.809150Z"
    }
   },
   "outputs": [],
   "source": [
    "# lora layer\n",
    "import math\n",
    "import warnings\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class LoraLayer:\n",
    "    def __init__(self, \n",
    "                 in_features: int,     # the number of input features\n",
    "                 out_features: int,):  # the number of output features\n",
    "        # Initializes dictionaries to store various parameters for each adapter in the layer\n",
    "        self.r = {}           # the rank of the low-rank matrix\n",
    "        self.lora_alpha = {}  # the scaling factor\n",
    "        self.scaling = {}     # the calculated scaling factor (lora_alpha / r)\n",
    "        \n",
    "        # Dropout layers for each adapter\n",
    "        self.lora_dropout = nn.ModuleDict({})\n",
    "        \n",
    "        # Weight matrices for the linear layers\n",
    "        self.lora_A = nn.ModuleDict({})\n",
    "        self.lora_B = nn.ModuleDict({})\n",
    "        \n",
    "        # Weight matrices for the embedding layers\n",
    "        self.lora_embedding_A = nn.ParameterDict({})\n",
    "        self.lora_embedding_B = nn.ParameterDict({})\n",
    "        \n",
    "        # Boolean flag indicating whether the weights ahve been merged\n",
    "        self.merged = False\n",
    "        \n",
    "        # Boolean flag indicating whether the adapters are disabled\n",
    "        self.disabled_adapters = False\n",
    "        \n",
    "        # Stores the number of input and output features\n",
    "        self.in_features =  in_features\n",
    "        self.out_feature = out_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c84376ab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-05T07:52:41.969577Z",
     "start_time": "2024-07-05T07:52:41.964706Z"
    }
   },
   "outputs": [],
   "source": [
    "# lora model\n",
    "class LoraModel(nn.Module):\n",
    "    def __init__(self, model, config, adapter_name='default'):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.forward = self.model.forward\n",
    "        self.peft_config = config\n",
    "        self.add_adapter(adapter_name, self.peft_config[adapter_name])\n",
    "    \n",
    "    def add_adapter(self, adapter_name, config=None): pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69cfe9a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-05T07:47:59.339065Z",
     "start_time": "2024-07-05T07:47:59.335108Z"
    }
   },
   "source": [
    "### train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb90cbe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-05T07:48:21.155594Z",
     "start_time": "2024-07-05T07:48:21.152109Z"
    }
   },
   "source": [
    "### inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "83280493",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-05T07:53:01.045642Z",
     "start_time": "2024-07-05T07:53:01.039657Z"
    }
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "\n",
    "import torch\n",
    "from transformers import GenerationConfig, AutoModelForCausalLM, AutoTokenizer, AutoConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "cb2e4bb4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-05T07:54:43.164566Z",
     "start_time": "2024-07-05T07:54:43.160327Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_response(prompt, tokenizer, model, generation_config, max_new_tokens):\n",
    "    inputs = tokenizer(prompt, return_tensors='pt')\n",
    "    output = model.generate(\n",
    "        input_ids = inputs['input_ids']\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
